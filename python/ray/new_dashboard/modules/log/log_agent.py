import logging
import os
import re
from time import time
import psutil
from ray.new_dashboard.modules.job import job_consts

import ray.new_dashboard.modules.log.log_utils as log_utils
import ray.new_dashboard.modules.log.log_consts as log_consts
import ray.new_dashboard.utils as dashboard_utils
from ray.new_dashboard.utils import async_loop_forever
logger = logging.getLogger(__name__)
routes = dashboard_utils.ClassMethodRouteTable

DRIVER_LOG_PATTERN_RE = re.compile(".*driver-([a-f0-9]+)[._]")
WORKER_LOG_PATTERN_RE = re.compile(
    ".*(?:pid|-|_)(\d+)(?:\.\d)?\.(?:out|err|log)")
# core-worker-{job_id}-{pid}.log
CORE_WORKER_LOG_PATTERN_RE = \
    re.compile(".*(?:python|java|cpp)-core-worker-([a-f0-9]+)[-_](\d+)\.log")
WHITELIST_LOG_PATTERN_RE = \
    re.compile(".*(?:jvm_gc_pid|event_CORE_WORKER_)")


class LogFilesGroup():
    def __init__(self, log_source, source_id, log_file_list, last_active_time):
        self.log_source = log_source
        self.source_id = source_id
        self.log_file_list = log_file_list
        self.last_active_time = last_active_time


class LogAgent(dashboard_utils.DashboardAgentModule):
    def __init__(self, dashboard_agent):
        super().__init__(dashboard_agent)
        log_utils.register_mimetypes()
        routes.static("/logs", self._dashboard_agent.log_dir, show_index=True)

    async def run(self, server):
        if log_consts.RAY_LOG_CLEANUP_ENABLED:
            await self._clean_outdated_logs(
                self._dashboard_agent.log_dir,
                os.path.join(self._dashboard_agent.log_dir, "events"),
                job_consts.JOB_ROOT_DIR.format(
                    temp_dir=self._dashboard_agent.temp_dir),
                log_consts.RAY_MAXIMUM_NUMBER_OF_LOGS_TO_KEEP,
                log_consts.RAY_LOGS_TTL_SECONDS,
                log_consts.RAY_PER_JOB_DEAD_PROCESS_LOG_NUMBER_LIMIT)

    async def _remove_log_files(self, log_source, source_id, log_file_paths):
        deleted_file_count = 0
        for file_path in log_file_paths:
            try:
                os.remove(file_path)
                logger.info("Removed log file: %s", file_path)
                deleted_file_count += 1
            except IsADirectoryError:
                logger.error("Trying to delete a directory %s", file_path)
            except FileNotFoundError:
                pass
        logger.info("Removed %d old logs of %s: %s", deleted_file_count,
                    log_source, source_id)

    def _job_dir_not_deleted(self, jobs_dir, job_id):
        job_dir = os.path.join(jobs_dir, job_id)
        return os.path.exists(job_dir)

    # Remove logs older the time threshold
    async def _remove_logs_by_time_threshold(self, log_source, id_to_paths_map,
                                             time_threshold):
        # list of LogFilesGroups
        remaining_log_groups = []
        for _id, file_paths in id_to_paths_map.items():
            last_active_time = max(
                [os.path.getmtime(file_path) for file_path in file_paths])
            # Remove logs older than the threshold
            if last_active_time < time_threshold:
                await self._remove_log_files(log_source, _id, file_paths)
                continue
            remaining_log_groups.append(
                LogFilesGroup(log_source, _id, file_paths, last_active_time))
        return remaining_log_groups

    # Remove logs if number list is reached
    async def _remove_logs_by_number_limit(self, log_group_list, number_limit):
        remaining_log_number = 0
        for log_group in log_group_list:
            remaining_log_number += len(log_group.log_file_list)
        if remaining_log_number <= number_limit:
            return log_group_list
        # Remove logs if remaining number is higher than the number threshold
        # Sort log groups by last active time
        log_group_list.sort(key=lambda log_group: log_group.last_active_time)
        for i in range(len(log_group_list)):
            log_group = log_group_list[i]
            await self._remove_log_files(log_group.log_source,
                                         log_group.source_id,
                                         log_group.log_file_list)
            remaining_log_number -= len(log_group.log_file_list)
            if remaining_log_number <= number_limit:
                break
        if i < len(log_group_list) - 1:
            return log_group_list[i + 1:]
        return []

    @async_loop_forever(log_consts.RAY_LOG_CLEANUP_INTERVAL_IN_SECONDS)
    async def _clean_outdated_logs(self, log_dir, event_dir, jobs_dir,
                                   global_log_number_limit, log_live_seconds,
                                   single_job_log_number_limit):
        """
        Delete outdated log files and generated by dead drivers and workers.

        This function go through the dashboard agent's log directory and
        search for logs generated by job drivers and job workers. It identifies
        logs using regex pattern and groups them by job id or pid extracted
        from the logs' file names.

        In each group of the log files, if the most recently modified file
        is modified before the designated threshold, all files in the group
        will be deleted.

        Worker logs are further grouped by their job id, single job's worker
        logs has a number limit defined by single_job_log_number_limit.

        If total file number is large than designated log file number threshold
        defined by global_log_number_limit, log groups are deleted in a last
        modified last delete fashion.
        """
        logger.info("Start cleaning logs in %s and %s", log_dir, event_dir)
        logger.info(
            "global log number limit is %d, " +
            "single job log number limit is %d, log live seconds is %d",
            global_log_number_limit, single_job_log_number_limit,
            log_live_seconds)

        # Find logs to clean
        log_file_names = os.listdir(log_dir)
        event_file_names = os.listdir(event_dir)
        log_file_paths = {os.path.join(log_dir, f) for f in log_file_names} \
            .union({os.path.join(event_dir, f) for f in event_file_names})

        # Store logs
        # {job_id:[log_file_path...]}
        dead_driver_logs = {}
        # {pid:[log_file_path...]}
        dead_worker_logs = {}
        # sets to screen out alive jobs/workers
        alive_process_ids = set()
        alive_job_ids = set()
        core_worker_pids = set()
        whitelist_worker_pids = set()
        pid_to_jid_map = {}

        for file_path in log_file_paths:
            driver_matcher = DRIVER_LOG_PATTERN_RE.match(file_path)
            if driver_matcher:
                # Make dead jobs' drivers log available for delete
                job_id = driver_matcher.group(1)
                if job_id in alive_job_ids:
                    continue
                if self._job_dir_not_deleted(jobs_dir, job_id):
                    alive_job_ids.add(job_id)
                    continue
                # Record dead jobs' log files
                dead_driver_logs.setdefault(job_id, []).append(file_path)
                continue

            worker_matcher = WORKER_LOG_PATTERN_RE.match(file_path)
            if worker_matcher:
                # Make dead workers' logs available for delete
                pid = worker_matcher.group(1)
                # Skip alive processes
                if pid in alive_process_ids:
                    continue
                if psutil.pid_exists(int(pid)):
                    alive_process_ids.add(pid)
                    continue
                core_worker_matcher = \
                    CORE_WORKER_LOG_PATTERN_RE.match(file_path)
                if core_worker_matcher:
                    job_id = core_worker_matcher.group(1)
                    core_worker_pids.add(pid)
                    pid_to_jid_map[pid] = job_id
                if WHITELIST_LOG_PATTERN_RE.match(file_path):
                    whitelist_worker_pids.add(pid)
                # Record dead processes' log files
                dead_worker_logs.setdefault(pid, []).append(file_path)

        # Remove logs generated by processes without a core worker
        for pid in dead_worker_logs.keys() - \
                core_worker_pids.union(whitelist_worker_pids):
            del dead_worker_logs[pid]

        # Group worker logs by job
        job_id_to_dead_worker_logs = {}
        for pid, file_paths in dead_worker_logs.items():
            job_id = pid_to_jid_map.get(pid, "unknown")
            job_id_to_dead_worker_logs.setdefault(job_id, {})[pid] = file_paths

        time_threshold = time() - log_live_seconds
        overall_log_groups = []
        for job_id, worker_logs in job_id_to_dead_worker_logs.items():
            # Clean outdated worker logs
            per_job_log_groups = await self._remove_logs_by_time_threshold(
                "worker", worker_logs, time_threshold)
            if job_id != "unknown":
                # Check and clean single job's worker log number
                per_job_log_groups = await self._remove_logs_by_number_limit(
                    per_job_log_groups, single_job_log_number_limit)
            overall_log_groups.extend(per_job_log_groups)

        # Clean outdated driver logs
        driver_log_groups = await self._remove_logs_by_time_threshold(
            "driver of job", dead_driver_logs, time_threshold)
        overall_log_groups.extend(driver_log_groups)

        # Clean logs exceed global log number limit
        await self._remove_logs_by_number_limit(overall_log_groups,
                                                global_log_number_limit)
        logger.info("End cleaning logs in %s and %s", log_dir, event_dir)
